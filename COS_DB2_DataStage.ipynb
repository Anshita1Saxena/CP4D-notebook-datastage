{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Integration between IBM CP4D Notebook of Analytics Project and DataStage of Transformation Project\n","\n","## Libraries and Connection import"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"b1984059-f644-492a-a982-989ec1869e4c"},"outputs":[],"source":["# @hidden_cell\n","\n","# Importing project to access connections\n","from project_lib import Project\n","\n","project = Project.access()\n","\n","# Fetching the COS details\n","cos_con_metadata = project.get_connection(name=\"IBM_COS_CONN\")\n","# Fetching the DB2 details\n","db2_con_metadata = project.get_connection(name=\"DB2_CONN\")\n","\n","# Importing all the header files\n","# File processing tools\n","import pandas as pd\n","import io\n","from io import StringIO\n","import datetime\n","# COS libraries\n","import ibm_boto3\n","from botocore.client import Config\n","# DB2 Warehouse libraries\n","import ibm_db\n","import ibm_db_dbi\n","# Library for calling DataStage API\n","import requests\n","import base64\n","\n","# Global Constant Variables\n","global COL_COUNT_CON\n","global ROW_COUNT_CON\n","COL_COUNT_CON = 0\n","ROW_COUNT_CON = 0"]},{"cell_type":"markdown","metadata":{},"source":["## Creating connections\n","IBM COS and DB2 connections are created in the code below.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0339b186c52040d18ec1458cbb69ddcd"},"outputs":[],"source":["def create_connections(cos_con_metadata, db2_con_metadata):\n","    # Connecting to COS\n","    client_cos_con = ibm_boto3.client(\n","        service_name='s3',\n","        ibm_api_key_id=cos_con_metadata['api_key'],\n","        ibm_auth_endpoint=cos_con_metadata['iam_url'],\n","        config=Config(signature_version='oauth'),\n","        endpoint_url=cos_con_metadata['url']\n","    )\n","\n","    # Connecting to DB2\n","    db2_url = \"DATABASE=\" + db2_con_metadata['database'] + \";HOSTNAME=\" \\\n","              + db2_con_metadata['host'] + \";PORT=\" \\\n","              + db2_con_metadata['port'] \\\n","              + \";PROTOCOL=TCPIP;QUERYTIMEOUTINTERVAL=0;UID=\" \\\n","              + db2_con_metadata['username'] + \";PWD=\" \\\n","              + db2_con_metadata['password'] + \";\"\n","    conn = ibm_db.connect(db2_url, \"\", \"\")\n","    connect = ibm_db_dbi.Connection(conn)\n","    return conn, connect, client_cos_con"]},{"cell_type":"markdown","metadata":{},"source":["## Parsing the configuration\n","The configuration file kept in COS Bucket. The configuration values will be entered by users in this file."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"723f2d01f942488b88383340f0c9e98c"},"outputs":[],"source":["def parse_config_file(bucket_name):\n","    # Configuration file is stored in IBM COS having user entered file details\n","    streaming_body_1 = client_cos_con.get_object(\n","        Bucket=bucket_name,\n","        Key='CONFIG/CONFIGURATION.INI')['Body']\n","    for items in streaming_body_1:\n","        items = str(items).replace(\"b'\", \"\").replace(\"'\", \"\")\n","        # Extracting values entered by user\n","        client_name, file_name, target_table_name, dsjob, proj_name, userid, \\\n","            passwd = items.split(\"|\")\n","    return \\\n","        client_name, file_name, target_table_name, dsjob, \\\n","        proj_name, userid, passwd"]},{"cell_type":"markdown","metadata":{},"source":["## File Operations\n","This function performs:\n","1. Validation on user file.\n","2. Save the success and failure status in DB2 audit table.\n","3. Call the datastage API.\n","4. Call the function to store the validated file into IBM COS."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1e3a18a257834de294262ea3493bbadb"},"outputs":[],"source":["def file_operations(\n","        bucket_name, file_name, schema_name, table_name, conn,\n","        dsjob, proj_name, userid, passwd, COL_COUNT_CON,\n","        ROW_COUNT_CON):\n","    # Load File in dataframe to perform operations\n","    obj = client_cos_con.get_object(Bucket=bucket_name, Key=file_name)\n","\n","    # Display all the digits after decimal\n","    pd.options.display.float_format = '{:.9f}'.format\n","    content = obj['Body'].read()\n","\n","    # Load the file into dataframe\n","    load_df = pd.read_csv(\n","        io.BytesIO(content), sep='\\t', dtype=object, low_memory=False,\n","        float_precision='round_trip')\n","\n","    # sysibm.syscolumns holds all the colum names for all tables\n","    column_count_sql = \"select count(*) from sysibm.syscolumns where \" \\\n","                       \"tbcreator = '\" + schema_name + \"' and tbname = '\" \\\n","                       + table_name + \"';\"\n","    stmt = ibm_db.exec_immediate(conn, column_count_sql)\n","    result = ibm_db.fetch_assoc(stmt)\n","\n","    # '1' index holds the column count\n","    target_table_col_count = result['1']\n","\n","    # Check whether target table and file column count is matching or not\n","    if str(target_table_col_count) == str(len(load_df.columns.values)):\n","        print(\"Equal Column count between file and table!\")\n","    else:\n","        print(\"Unequal column count between file and table!\")\n","        error_description_1 = \"Unequal column count between file and table!\"\n","        COL_COUNT_CON = 1\n","\n","    \"\"\"\n","    Extract the number of lines from the last line\n","    Last line in each file holds the number of rows entered by user\n","    \"\"\"\n","    num_of_lines = str(\n","        load_df[load_df.columns[0]].iloc[-1]).replace(\"\\\n","                        TRAILER: \", \"\").replace(\" Data Records\", \"\")\n","\n","    # Get the number of rows from dataframe, i.e., file\n","    len_of_df = len(load_df) - 1\n","\n","    # Comparison of user provided last line and number of rows\n","    if str(num_of_lines) == str(len_of_df):\n","        print(\"Equal number of rows between file rows and Trailer count!\")\n","    else:\n","        print(\"Unequal number of rows between file rows and Trailer count!\")\n","        error_description_2 = \"Unequal number of rows between file rows and\\\n","                                Trailer count!\"\n","        ROW_COUNT_CON = 1\n","\n","    # Fetch the maximum id from audit status table\n","    check_rows = \"select max(ID) from demo.audit_status;\"\n","    stmt = ibm_db.exec_immediate(conn, check_rows)\n","    result = ibm_db.fetch_assoc(stmt)\n","    audit_rows = 0 if result['1'] is None else result['1']\n","    id = int(audit_rows) + 1\n","\n","    # Extract client name from file name\n","    client_name = (str(file_name).split(\"_\")[0]).replace(\"/landing\", \"\")\n","\n","    # Extract file name\n","    file_tab_name = str(file_name).split(\"/\")[2]\n","\n","    # Value of today's date\n","    today = datetime.datetime.now()\n","    etl_ld_dt = today.strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","    \"\"\"\n","    Checking whether the column count and row count matches or not.\n","    If not, then do not perform audit operations.\n","    \"\"\"\n","    if COL_COUNT_CON == 0 and ROW_COUNT_CON == 0:\n","        # Insert values in audit table\n","        insert_audit_sql = \"insert into demo.audit_table (ID,CLIENT_NAME,\\\n","                                RECON_LYR,TABLE_NM,METRIC_TYP,METRIC_VAL,\\\n","                                ETL_LD_DT) values (\" + str(id) + \", '\" \\\n","                           + client_name + \"', 'SOURCE', '\" \\\n","                           + file_tab_name + \"', 'COUNT', \" \\\n","                           + num_of_lines + \", '\" + etl_ld_dt + \"');\"\n","        insert_audit_stmt = ibm_db.prepare(conn, insert_audit_sql)\n","        ibm_db.execute(insert_audit_stmt)\n","\n","        \"\"\"\n","        Insert the success status of file in audit table\n","        if no discrepancy exist in row and column count.\n","        \"\"\"\n","        insert_batch_audit_sql = \"insert into demo.audit_table \\\n","                                    (ID,CLIENT_ID,DATA_LYR,JOB_NAME,\\\n","                                    ETL_LD_DT,JOB_STATUS) values (\" + str(id) \\\n","                                 + \", '\" + client_name + \"', 'SOURCE', '\" \\\n","                                 + file_tab_name + \"', '\" + etl_ld_dt \\\n","                                 + \"', 'SUCCESS');\"\n","        insert_batch_audit_stmt = ibm_db.prepare(conn, insert_batch_audit_sql)\n","        ibm_db.execute(insert_batch_audit_stmt)\n","\n","        # Strip off the trailer record line from the file\n","        load_df.drop(load_df.tail(1).index, inplace=True)\n","\n","        # Load dataframe into COS\n","        store_file_name = str(file_name).split(\"/\")[2]\n","        storepath = client_name + \"/processed_zone/\" + store_file_name\n","\n","        # call function to load the dataframe to IBM COS\n","        copy_to_s3(client_cos_con, load_df, bucket_name, storepath)\n","\n","        # Call the datastage api to run the internal transformation command\n","        call_dsjob_api(dsjob, proj_name, userid, passwd)\n","    elif COL_COUNT_CON == 1 or ROW_COUNT_CON == 1:\n","        \"\"\"\n","        Insert the failure status of file in audit table\n","        if discrepancy exist in row and column count.\n","        \"\"\"\n","        # Unmatched Column Count\n","        if COL_COUNT_CON == 1:\n","            insert_batch_audit_sql_1 = \"insert into demo.audit_table (ID,\\\n","                                            CLIENT_ID,DATA_LYR,JOB_NAME,\\\n","                                            ETL_LD_DT,JOB_STATUS,DESCRIPTION)\\\n","                                            values (\" + str(id) + \", '\" \\\n","                                       + client_name + \"', 'SOURCE', '\" \\\n","                                       + file_tab_name + \"', '\" + etl_ld_dt \\\n","                                       + \"', 'FAILURE', '\" \\\n","                                       + error_description_1 + \"');\"\n","            insert_batch_audit_stmt_1 = ibm_db.prepare(\n","                conn, insert_batch_audit_sql_1)\n","            ibm_db.execute(insert_batch_audit_stmt_1)\n","\n","        # Unmatched row count\n","        if ROW_COUNT_CON == 1:\n","            insert_batch_audit_sql_2 = \"insert into demo.audit_table (ID,\\\n","                                            CLIENT_ID,DATA_LYR,JOB_NAME,\\\n","                                            ETL_LD_DT,JOB_STATUS,DESCRIPTION)\\\n","                                            values (\" + str(id) + \", '\" \\\n","                                       + client_name + \"', 'SOURCE', '\" \\\n","                                       + file_tab_name + \"', '\" + etl_ld_dt \\\n","                                       + \"', 'FAILURE', '\" \\\n","                                       + error_description_2 + \"');\"\n","            insert_batch_audit_stmt_2 = ibm_db.prepare(\n","                conn, insert_batch_audit_sql_2)\n","            ibm_db.execute(insert_batch_audit_stmt_2)\n","        print(\"File is failed due to above discrepancy!\")"]},{"cell_type":"markdown","metadata":{},"source":["## Copy file to IBM COS\n","This function write the content as CSV file in IBM COS."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"b9c3b2747d8443b881c1229a4b11e464"},"outputs":[],"source":["def copy_to_s3(client, df, bucket, filepath):\n","    # Write CSV file into buffer\n","    csv_buf = StringIO()\n","    df.to_csv(\n","        csv_buf, header=True, index=False, sep='\\t', encoding='utf-8',\n","        quoting=1)\n","    csv_buf.seek(0)\n","\n","    # Put the csv buffer dataframe into the IBM COS bucket path\n","    client.put_object(Bucket=bucket, Body=csv_buf.getvalue(), Key=filepath)\n","    print(\n","        f'Copy {df.shape[0]} rows to S3 Bucket {bucket} at {filepath}, Done!')"]},{"cell_type":"markdown","metadata":{},"source":["## Call DataStage API\n","The below function will call DataStage API for running the Transformation Job."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"943f52e373094bf1821baa92c47d3f6b"},"outputs":[],"source":["def call_dsjob_api(dsjob, proj_name, userid, passwd):\n","    try:\n","\n","        # Creating a syntax for an API Call\n","        # CP4D_URL is the always static value\n","        CP4D_URL = \"https://xxx.xxxx.com\"\n","\n","        # Creation of JOB API format\n","        CP4D_JOB_API = \"/ibm/iis/api/dscdesignerapi?api={apiName}&jobName\" \\\n","                       \"={jobName}&projectName={projectName}&hostName\" \\\n","                       \"=is-en-conductor-0.en-cond&getFullOutput\" \\\n","                       \"={getFullOutput}\"\n","\n","        # Creation of Data Stage API for running job\n","        jobAPI = CP4D_URL + CP4D_JOB_API\n","        jobRunURL = jobAPI.format(\n","            apiName='runDSJob', jobName=dsjob,\n","            projectName=proj_name, getFullOutput=\"true\")\n","\n","        # Call Data Stage API\n","        contentType = \"application/json\"\n","        basic_user_and_pasword = base64.b64encode(\n","            '{}:{}'.format(\n","                userid,\n","                passwd).encode(\n","                'utf-8'))\n","        httpSession = requests.Session()\n","        response = httpSession.post(\n","            jobRunURL,\n","            headers={\n","                \"Content-Type\": contentType,\n","                \"Authorization\": \"Basic \" +\n","                                 basic_user_and_pasword.decode('utf-8')})\n","\n","        # Printing a message for User Satisfaction\n","        print(\"Data Stage job is Running!\")\n","        responseData = response.json()\n","\n","        # If response is not succeeded, then print the error\n","        if response.status_code != 200:\n","            print(\n","                \"Error occurred. Http status code is \" +\n","                str(response.status_code))\n","\n","        \"\"\"\n","        If response is not succeeded then check\n","        that Job Status is FWF which means job is aborted\n","        and need re-compilation.\n","        \"\"\"\n","        if not responseData[\"succeeded\"]:\n","            if str(responseData[\"JobStatus\"]) in {'FWF'}:\n","                print(\n","                    \"The job is not in a runnable state, it may need to be\"\n","                    \" re-compiled. Its current state is 'Aborted'. \"\n","                    \"Job is re-compiled!\")\n","\n","                # Creation of Data Stage API for compiling job\n","                jobCompileURL = jobAPI.format(\n","                    apiName='compileDSJob',\n","                    jobName=dsjob,\n","                    projectName=proj_name,\n","                    getFullOutput=\"true\")\n","\n","                # Call Data Stage API for compiling job\n","                httpSession.post(jobCompileURL, headers={\n","                    \"Content-Type\": contentType,\n","                    \"Authorization\": \"Basic \" +\n","                                     basic_user_and_pasword.decode(\n","                                                                'utf-8')})\n","                print(\"Process is finished as Job is re-compiling!\")\n","\n","        # Job Status API\n","        if str(responseData[\"JobStatus\"]) not in {'FWF'}:\n","\n","            # If the job is not in aborted status then check its status.\n","            jobStatusURL = jobAPI.format(\n","                apiName='getDSJobStatus', jobName=dsjob,\n","                projectName=proj_name,\n","                getFullOutput=\"true\")\n","            contentType = \"application/json\"\n","            httpSession = requests.Session()\n","            response = httpSession.post(jobStatusURL, headers={\n","                \"Content-Type\": contentType,\n","                \"Authorization\": \"Basic \" +\n","                                 basic_user_and_pasword.decode(\n","                                                            'utf-8')})\n","            responseData = response.json()\n","\n","            \"\"\"\n","            If job response status code is not succeeded then print error\n","            status.\n","            \"\"\"\n","            if response.status_code != 200:\n","                print(\n","                    \"Error occurred. Http status code is \" +\n","                    str(response.status_code))\n","            else:\n","                responseData = response.json()\n","\n","            \"\"\"\n","            Recursive function for checking job Status for Running (\"RUN\",\n","            \"RNW\", \"RNF\", \"RNS\") and compiling (\"FOK\", \"CMP\")\n","            Condition run = 1 for recursive function.\n","            This function is polling DataStage.\n","            This is a nested function and working as an Encapsulation.\n","            \"\"\"\n","\n","            def call_api_func(\n","                    jobStatusURL, contentType, basic_user_and_pasword):\n","                response = httpSession.post(jobStatusURL, headers={\n","                    \"Content-Type\": contentType,\n","                    \"Authorization\": \"Basic \" +\n","                                     basic_user_and_pasword.decode(\n","                                                                'utf-8')})\n","                return response\n","\n","            run = 1\n","            if \"JobStatus\" in responseData.keys():\n","                # Check if job is running or compiling\n","                if str(responseData[\"JobStatus\"]) in {\n","                                            \"RUN\", \"RNW\", \"RNF\",\n","                                            \"RNS\", \"CMP\"}:\n","                    run = 0\n","                    while run == 0:\n","                        response = call_api_func(\n","                            jobStatusURL, contentType,\n","                            basic_user_and_pasword)\n","                        responseData = response.json()\n","                        \"\"\"\n","                        If the status is not Run or Compile,\n","                        then give the job status as succeeded, failed or\n","                        aborted\n","                        \"\"\"\n","                        if str(responseData[\"JobStatus\"]) not in {\n","                                                        \"RUN\", \"RNW\", \"RNF\",\n","                                                        \"RNS\", \"CMP\"}:\n","                            responseData = response.json()\n","                            run = 1\n","\n","                # Print the status of Job\n","                if str(responseData[\"JobStatus\"]) == \"FWW\":\n","                    print(\"Job is Finished with Warnings\")\n","                elif str(responseData[\"JobStatus\"]) in {\"SUCCESS\", \"FOK\"}:\n","                    print(\"Job is Succeeded!\")\n","                elif str(responseData[\"JobStatus\"]) == \"FAILED\":\n","                    print(\"Job is Failed!\")\n","                elif str(responseData[\"JobStatus\"]) == \"FWF\":\n","                    print(\"Job is Failed - Aborted!\")\n","            else:\n","                print(responseData)\n","\n","    except Exception as e:\n","        print(\"Error Occurred.\")\n","        print(e)"]},{"cell_type":"markdown","metadata":{},"source":["## Main Cell\n","Program executes from here. This cell will call all the required functions for performing the job.\n","\n","### User Inputs\n","1. **client_name:** Name of client of this file.\n","1. **file_name:** Name of file that needs to be processed.\n","2. **target_table_name:** Name of the table where this file will be uploaded.\n","3. **dsjob:** Name of Job that needs to be run for transformation on the data of this file.\n","4. **proj_name:** Name of the DataStage Transformation project.\n","5. **userid:** User ID of the user on this IBM CP4D Cluster.\n","6. **passwd:** Password of the user on this IBM CP4D Cluster."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"31e94eb8da57409195280449357c73fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing File: BANK/landing_zone/BANK_LAT_TAB of client BANK\n","Equal Column count between file and table!\n","Equal number of rows between file rows and Trailer count!\n","insert into demo.audit_table (ID,CLIENT_ID,RECON_LYR,TABLE_NM,METRIC_TYP,METRIC_VAL,ETL_LD_DT) values (18, 'BANK', 'SOURCE', 'BANK_LAT_TAB', 'COUNT', 49559, '2021-10-19 13:52:08');\n","Copy 49559 rows to S3 Bucket demo-files at BANK/processed_zone/BANK_LAT_TAB, Done!\n","Data Stage job is Running!\n","Job is Succeeded!\n"]}],"source":["# Name of the bucket of s3 (IBM COS)\n","bucket_name = 'demo-files'\n","\n","# Calling function to create connections\n","conn, connect, client_cos_con = create_connections(\n","    cos_con_metadata, db2_con_metadata)\n","\n","# Calling functions to parse the user provided file\n","client_name, file_name, target_table_name, dsjob, proj_name, userid, \\\n","    passwd = parse_config_file(bucket_name)\n","\n","# Constructing file name\n","file_name = client_name + '/landing_zone/' + file_name\n","\n","# Extract the schema and table names\n","schema_name, table_name = target_table_name.split(\".\")\n","\n","# If user provided all the necessary values, call the main processing function\n","if client_name is not None and file_name is not None \\\n","        and target_table_name is not None \\\n","        and dsjob is not None \\\n","        and proj_name is not None \\\n","        and userid is not None \\\n","        and passwd is not None:\n","    print(\n","        \"Processing File: \" + str(file_name) +\n","        \" of client \" + str(client_name))\n","    file_operations(\n","        bucket_name, file_name, schema_name, table_name, conn,\n","        dsjob, proj_name, userid, passwd, COL_COUNT_CON, ROW_COUNT_CON)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0758475977fc4cd489bbdb124c185084"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.7","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":1}
